<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><title>C4. Kubernetes Architecture - Kubernetes Notes</title><link rel="shortcut icon" href="http://localhost:5000/favicon.png" type="image/x-icon"><link rel="stylesheet" href="http://localhost:5000/assets/css/just-the-docs.css"> <script type="text/javascript" src="http://localhost:5000/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="http://localhost:5000/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>C4. Kubernetes Architecture | Kubernetes Notes</title><meta name="generator" content="Jekyll v3.8.5" /><meta property="og:title" content="C4. Kubernetes Architecture" /><meta property="og:locale" content="en_US" /><meta name="description" content="Includes Introduction to Kubernetes Notes" /><meta property="og:description" content="Includes Introduction to Kubernetes Notes" /><link rel="canonical" href="http://localhost:5000/docs/chapter4/" /><meta property="og:url" content="http://localhost:5000/docs/chapter4/" /><meta property="og:site_name" content="Kubernetes Notes" /> <script type="application/ld+json"> {"url":"http://localhost:5000/docs/chapter4/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:5000/assets/images/kubernetes.svg"}},"headline":"C4. Kubernetes Architecture","description":"Includes Introduction to Kubernetes Notes","@type":"WebPage","@context":"https://schema.org"}</script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="link" viewBox="0 0 16 16"><title>Link</title><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path> </symbol> </svg><div class="page-wrap"><div class="side-bar"><div class="site-header"> <a href="http://localhost:5000/" class="site-title lh-tight"><div class="site-logo"></div></a> <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button></div><div class="navigation main-nav js-main-nav"><nav role="navigation" aria-label="Main navigation"><ul class="navigation-list"><li class="navigation-list-item"><a href="http://localhost:5000/" class="navigation-list-link">Introduction to Kubernetes</a><li class="navigation-list-item"><a href="http://localhost:5000/docs/chapter1/" class="navigation-list-link">C1.From Monolith to Microservices</a><li class="navigation-list-item"><a href="http://localhost:5000/docs/chapter2/" class="navigation-list-link">C2. Container Orchestration</a><li class="navigation-list-item"><a href="http://localhost:5000/docs/chapter3/" class="navigation-list-link">C3. Kubernetes</a><li class="navigation-list-item active"><a href="http://localhost:5000/docs/chapter4/" class="navigation-list-link active">C4. Kubernetes Architecture</a></ul></nav></div><footer class="site-footer"><p class="text-small text-grey-dk-000 mb-4">Includes Introduction to Kubernetes Notes</p></footer></div><div class="main-content-wrap js-main-content" tabindex="0"><div class="main-content"><div class="page-header js-page-header"><div class="search"><div class="search-input-wrap"> <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search Kubernetes Notes" aria-label="Search Kubernetes Notes" autocomplete="off"> <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg></div><div class="js-search-results search-results-wrap"></div></div><ul class="list-style-none text-small aux-nav"><li class="d-inline-block my-0"><a href="https://www.edx.org/course/introduction-to-kubernetes">Introduction to Kubernetes</a></ul></div><div class="page"><div id="main-content" class="page-content" role="main"><p>⚠️ <strong>Notice: With respect to licence (<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">https://creativecommons.org/licenses/by-nc-nd/4.0/</a>) on this course, I am distributing and sharing important points and some information which are taken from <a href="https://www.edx.org/course/introduction-to-kubernetes">https://www.edx.org/course/introduction-to-kubernetes</a> course</strong></p><ul><li><a href="#kubernetes-architecture">Kubernetes Architecture</a><ul><li><a href="#master-node">Master Node</a><ul><li><a href="#master-node-components-api-server">Master Node Components: API Server</a><li><a href="#master-node-components-scheduler">Master Node Components: Scheduler</a><li><a href="#master-node-components-controller-managers">Master Node Components: Controller Managers</a><li><a href="#master-node-components-etcd">Master Node Components: etcd</a></ul><li><a href="#worker-node">Worker Node</a><ul><li><a href="#worker-node-components-container-runtime">Worker Node Components: Container Runtime</a><li><a href="#worker-node-components-kubelet">Worker Node Components: kubelet</a><li><a href="#worker-node-components-kubelet---cri-shims">Worker Node Components: kubelet - CRI shims</a><li><a href="#worker-node-components-kube-proxy">Worker Node Components: kube-proxy</a><li><a href="#worker-node-components-addons">Worker Node Components: Addons</a></ul><li><a href="#networking-challenges">Networking Challenges</a><li><a href="#container-to-container-communication-inside-pods">Container-to-Container Communication Inside Pods</a><li><a href="#pod-to-pod-communication-across-nodes">Pod-to-Pod Communication Across Nodes</a><li><a href="#pod-to-external-world-communication">Pod-to-External World Communication</a></ul></ul><h1 id="kubernetes-architecture"> <a href="#kubernetes-architecture" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Kubernetes Architecture</h1><p>At a very high level, Kubernetes has the following main components:</p><ul><li>One or more <strong>master nodes</strong><li>One or more <strong>worker nodes</strong><li>Distributed key-value store, such as <a href="https://etcd.io/">etcd</a>.</ul><p><img src="../../assets/images/intro-kubernetes/kube-arch.png" alt="Kubernetes Architecture" /></p><h2 id="master-node"> <a href="#master-node" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Master Node</h2><p>The <strong>master node</strong> provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster’s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API).</p><p><img src="../../assets/images/intro-kubernetes/master_node.png" alt="Kubernetes Master Node" /></p><p>It is important to keep the control plane running at all costs. Losing the control plane may introduce downtimes, causing service disruption to clients, with possible loss of business. To ensure the control plane’s fault tolerance, master node replicas are added to the cluster, configured in High-Availability (HA) mode. While only one of the master node replicas actively manages the cluster, the control plane components stay in sync across the master node replicas. This type of configuration adds resiliency to the cluster’s control plane, should the active master node replica fail.</p><p>To persist the Kubernetes cluster’s state, all cluster configuration data is saved to <a href="https://github.com/etcd-io">etcd</a>. However, <strong>etcd</strong> is a distributed key-value store which only holds cluster state related data, no client workload data. etcd is configured on the master node (<a href="https://kubernetes.io/docs/setup/independent/ha-topology/#stacked-etcd-topology">stacked</a>) or on its dedicated host (<a href="https://kubernetes.io/docs/setup/independent/ha-topology/#external-etcd-topology">external</a>) to reduce the chances of data store loss by decoupling it from the control plane agents.</p><p>When stacked, HA master node replicas ensure etcd resiliency as well. Unfortunately, that is not the case of external etcds, when the etcd hosts have to be separately replicated for HA mode configuration.</p><p>A master node has the following components:</p><ul><li>API server<li>Scheduler<li>Controller managers<li>etcd.</ul><h3 id="master-node-components-api-server"> <a href="#master-node-components-api-server" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Master Node Components: API Server</h3><p>All the administrative tasks are coordinated by the <strong>kube-apiserver</strong>, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster’s current state from the etcd, and after a call’s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster’s data store.</p><p>The API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules.</p><h3 id="master-node-components-scheduler"> <a href="#master-node-components-scheduler" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Master Node Components: Scheduler</h3><p>The role of the <strong>kube-scheduler</strong> is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object’s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object’s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with <strong>disk==ssd</strong> key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc.</p><p>The scheduler is highly configurable and customizable. Additional custom schedulers are supported, then the object’s configuration data should include the name of the custom scheduler expected to make the scheduling decision for that particular object; if no such data is included, the default scheduler is selected instead.</p><p>A scheduler is extremely important and quite complex in a multi-node Kubernetes cluster. In a single-node Kubernetes cluster, such as the one explored later in this course, the scheduler’s job is quite simple.</p><h3 id="master-node-components-controller-managers"> <a href="#master-node-components-controller-managers" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Master Node Components: Controller Managers</h3><ul><li><p><strong>controller managers</strong></p><p>The <strong>controller managers</strong> are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster’s desired state (provided by objects’ configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state.</p><li><p><strong>kube-controller-manager</strong></p><p>The <strong>kube-controller-manager</strong> runs controllers responsible to act when nodes become unavailable, to ensure pod counts are as expected, to create endpoints, service accounts, and API access tokens.</p><li><p><strong>cloud-controller-manager</strong></p><p>The <strong>cloud-controller-manager</strong> runs controllers responsible to interact with the underlying infrastructure of a cloud provider when nodes become unavailable, to manage storage volumes when provided by a cloud service, and to manage load balancing and routing.</p></ul><h3 id="master-node-components-etcd"> <a href="#master-node-components-etcd" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Master Node Components: etcd</h3><p><strong>etcd</strong> is a distributed key-value data store used to persist a Kubernetes cluster’s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store.</p><p>Out of all the control plane components, only the API server is able to communicate with the etcd data store.</p><p>etcd’s CLI management tool provides backup, snapshot, and restore capabilities which come in handy especially for a single etcd instance Kubernetes cluster - common in Development and learning environments. However, in Stage and Production environments, it is extremely important to replicate the data stores in HA mode, for cluster configuration data resiliency.</p><p>etcd is based on the <a href="https://web.stanford.edu/~ouster/cgi-bin/papers/raft-atc14">Raft Consensus Algorithm</a> which allows a collection of machines to work as a coherent group that can survive the failures of some of its members. At any given time, one of the nodes in the group will be the master, and the rest of them will be the followers. Any node can be treated as a master.</p><p><img src="../../assets/images/intro-kubernetes/master-followers.png" alt="Master and Followers" /></p><p><strong>etcd</strong> is written in the <strong>Go programming language</strong>. In Kubernetes, besides storing the cluster state, etcd is also used to store configuration details such as subnets, ConfigMaps, Secrets, etc.</p><h2 id="worker-node"> <a href="#worker-node" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node</h2><p>Could be useful to check out <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/"><strong>POD</strong></a> definition.</p><p>A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together</p><p><img src="../../assets/images/intro-kubernetes/worker_node.png" alt="Worker Node" /></p><p>Also, to access the applications from the external world, we connect to worker nodes and not to the master node</p><p>A worker node has the following components:</p><ul><li>Container runtime<li>kubelet<li>kube-proxy<li>Addons for DNS, Dashboard, cluster-level monitoring and logging.</ul><h3 id="worker-node-components-container-runtime"> <a href="#worker-node-components-container-runtime" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node Components: Container Runtime</h3><p>Although Kubernetes is described as a “container orchestration engine”, it does not have the capability to directly handle containers. In order to run and manage a container’s lifecycle, Kubernetes requires a <strong>container runtime</strong> on the node where a Pod and its containers are to be scheduled. Kubernetes supports many container runtimes:</p><ul><li><p><a href="https://www.docker.com/"><strong>Docker</strong></a> - although a container platform which uses containerd as a container runtime, it is the most widely used container runtime with Kubernetes</p><li><p><a href="https://cri-o.io/"><strong>CRI-O</strong></a> - a lightweight container runtime for Kubernetes, it also supports Docker image registries</p><li><p><a href="https://containerd.io/"><strong>containerd</strong></a> - a simple and portable container runtime providing robustness</p><li><p><a href="https://github.com/rkt/rkt"><strong>rkt</strong></a> - a pod-native container engine, it also runs Docker images</p><li><p><a href="https://github.com/kubernetes-incubator/rktlet"><strong>rktlet</strong></a> - a Kubernetes <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Container Runtime Interface</a> (CRI) implementation using rkt</p></ul><h3 id="worker-node-components-kubelet"> <a href="#worker-node-components-kubelet" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node Components: kubelet</h3><p>The <strong>kubelet</strong> is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod’s running containers.</p><p>The kubelet connects to the container runtime using <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Container Runtime Interface</a> (CRI). CRI consists of protocol buffers, gRPC API, and libraries.</p><p><img src="../../assets/images/intro-kubernetes/cri.png" alt="Container Runtime Interface " /></p><p>As shown above, the kubelet acting as grpc client connects to the CRI shim acting as grpc server to perform container and image operations. CRI implements two services: <strong>ImageService</strong> and <strong>RuntimeService</strong>. The <strong>ImageService</strong> is responsible for all the image-related operations, while the <strong>RuntimeService</strong> is responsible for all the Pod and container-related operations.</p><p>Container runtimes used to be hard-coded in Kubernetes, but with the development of CRI, Kubernetes is more flexible now and uses different container runtimes without the need to recompile. Any container runtime that implements CRI can be used by Kubernetes to manage Pods, containers, and container images.</p><h3 id="worker-node-components-kubelet---cri-shims"> <a href="#worker-node-components-kubelet---cri-shims" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node Components: kubelet - CRI shims</h3><p>Below you will find some examples of CRI shims:</p><ul><li><p><strong>dockershim</strong></p><p>With dockershim, containers are created using Docker installed on the worker nodes. Internally, Docker uses containerd to create and manage containers.</p><p><img src="../../assets/images/intro-kubernetes/dockershim.png" alt="dockershim" /></p><li><p><strong>cri-containerd</strong></p><p>With cri-containerd, we can directly use Docker’s smaller offspring containerd to create and manage containers.</p><p><img src="../../assets/images/intro-kubernetes/cri-containerd.png" alt="cri-containerd" /></p><li><p><strong>CRI-O</strong></p><p>CRI-O enables using any Open Container Initiative (OCI) compatible runtimes with Kubernetes. At the time this course was created, CRI-O supported runC and Clear Containers as container runtimes. However, in principle, any OCI-compliant runtime can be plugged-in.</p><p><img src="../../assets/images/intro-kubernetes/crio.png" alt="CRI-O" /></p></ul><h3 id="worker-node-components-kube-proxy"> <a href="#worker-node-components-kube-proxy" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node Components: kube-proxy</h3><p>The <strong>kube-proxy</strong> is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods.</p><h3 id="worker-node-components-addons"> <a href="#worker-node-components-addons" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Worker Node Components: Addons</h3><p><strong>Addons</strong> are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.</p><ul><li><strong>DNS</strong> - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources<li><strong>Dashboard</strong> - a general purposed web-based user interface for cluster management<li><strong>Monitoring</strong> - collects cluster-level container metrics and saves them to a central data store<li><strong>Logging</strong> - collects cluster-level container logs and saves them to a central log store for analysis.</ul><h2 id="networking-challenges"> <a href="#networking-challenges" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Networking Challenges</h2><p>Decoupled microservices based applications rely heavily on networking in order to mimic the tight-coupling once available in the monolithic era. Networking, in general, is not the easiest to understand and implement. Kubernetes is no exception - as a containerized microservices orchestrator is needs to address 4 distinct networking challenges:</p><ul><li>Container-to-container communication inside Pods<li>Pod-to-Pod communication on the same node and across cluster nodes<li>Pod-to-Service communication within the same namespace and across cluster namespaces<li>External-to-Service communication for clients to access applications in a cluster.</ul><p>All these networking challenges must be addressed before deploying a Kubernetes cluster.</p><h2 id="container-to-container-communication-inside-pods"> <a href="#container-to-container-communication-inside-pods" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Container-to-Container Communication Inside Pods</h2><p>Making use of the underlying host operating system’s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A <strong>network namespace</strong> is shared across containers, or with the host operating system.</p><p>When a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost.</p><h2 id="pod-to-pod-communication-across-nodes"> <a href="#pod-to-pod-communication-across-nodes" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Pod-to-Pod Communication Across Nodes</h2><p>In a Kubernetes cluster Pods are scheduled on nodes randomly. Regardless of their host node, Pods are expected to be able to communicate with all other Pods in the cluster, all this without the implementation of Network Address Translation (NAT). This is a fundamental requirement of any networking implementation in Kubernetes.</p><p>The Kubernetes network model aims to reduce complexity, and it treats Pods as VMs on a network, where each VM receives an IP address - thus each Pod receiving an IP address. This model is called <strong>“IP-per-Pod”</strong> and ensures Pod-to-Pod communication, just as VMs are able to communicate with each other.</p><p>Let’s not forget about containers though. They share the Pod’s network namespace and must coordinate ports assignment inside the Pod just as applications would on a VM, all while being able to communicate with each other on <strong>localhost</strong> - inside the Pod.</p><p>However, containers are integrated with the overall Kubernetes networking model through the use of the <a href="https://github.com/containernetworking/cni">Container Network Interface (CNI)</a> supported by <a href="https://github.com/containernetworking/cni#3rd-party-plugins">CNI plugins</a>.</p><p>CNI is a set of a specification and libraries which allow plugins to configure the networking for containers. While there are a few <a href="https://github.com/containernetworking/plugins#plugins">core plugins</a>, most CNI plugins are 3rd-party Software Defined Networking (SDN) solutions implementing the Kubernetes networking model. In addition to addressing the fundamental requirement of the networking model, some networking solutions offer support for Network Policies. <a href="https://github.com/coreos/flannel/">Flannel</a>, <a href="https://www.weave.works/oss/net/">Weave</a>, <a href="https://www.projectcalico.org/">Calico</a> are only a few of the SDN solutions available for Kubernetes clusters.</p><p><img src="../../assets/images/intro-kubernetes/cni.png" alt="Container Network Interface CNI" /></p><p>The container runtime offloads the IP assignment to CNI, which connects to the underlying configured plugin, such as Bridge or MACvlan, to get the IP address. Once the IP address is given by the respective plugin, CNI forwards it back to the requested container runtime.</p><p>For more details, you can explore the <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">Kubernetes documentation</a>.</p><h2 id="pod-to-external-world-communication"> <a href="#pod-to-external-world-communication" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Pod-to-External World Communication</h2><p>For a successfully deployed containerized applications running in Pods inside a Kubernetes cluster, it requires accessibility from the outside world. Kubernetes enables external accessibility through <strong>services</strong>, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with <strong>kube-proxy</strong>, applications become accessible from outside the cluster over a virtual IP.</p></div></div></div></div>
