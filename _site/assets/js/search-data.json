{
  
  "1": {
    "title": "C1.From Monolith to Microservices",
    "content": "⚠️ Notice: With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . From Monolith to Microservices The Legacy Monolith | The Modern Service | Refactoring | Challenges | Success Stories | . | . From Monolith to Microservices . One application to many application in many places which are accesible in any time in any size. . The Legacy Monolith . Monolithic applications with all components tightly coupled and almost impossible to separate, a nightmare to manage and deployed on super expensive hardware. . Although most enterprises believe that the cloud will be the new home for legacy apps, not all legacy apps are a fit for the cloud, at least not yet. . Moving an application to the cloud should be as easy as walking on the beach and collecting pebbles in a bucket and easily carry them wherever needed. A 1000-ton boulder, on the other hand, is not easy to carry at all. This boulder represents the monolith application - sedimented layers of features and redundant logic translated into thousands of lines of code, written in a single, not so modern programming language, based on outdated software architecture patterns and principles. . In time, the new features and improvements added to code complexity, making development more challenging - loading, compiling, and building times increase with every new update. However, there is some ease in administration as the application is running on a single server, ideally a Virtual Machine or a Mainframe. . A monolith has a rather expensive taste in hardware. Being a large, single piece of software which continuously grows, it has to run on a single system which has to satisfy its compute, memory, storage, and networking requirements. The hardware of such capacity is both complex and pricey. . Since the entire monolith application runs as a single process, the scaling of individual features of the monolith is almost impossible. It internally supports a hardcoded number of connections and operations. However, scaling the entire application means to manually deploy a new instance of the monolith on another server, typically behind a load balancing appliance - another pricey solution. . During upgrades, patches or migrations of the monolith application - downtimes occur and maintenance windows have to be planned as disruptions in service are expected to impact clients. While there are solutions to minimize downtimes to customers by setting up monolith applications in a highly available active/passive configuration, it may still be challenging for system engineers to keep all systems at the same patch level. . The Modern Service . Microservices can be deployed individually on separate servers provisioned with fewer resources - only what is required by each service and the host system itself. . Microservices-based architecture is aligned with Event-driven Architecture and Service-Oriented Architecture (SOA) principles, where complex applications are composed of small independent processes which communicate with each other through APIs over a network. APIs allow access by other internal services of the same application or external, third-party services and applications. . Although the distributed nature of microservices adds complexity to the architecture, one of the greatest benefits of microservices is scalability. With the overall application becoming modular, each microservice can be scaled individually, either manually or automated through demand-based autoscaling. . Seamless upgrades and patching processes are other benefits of microservices architecture. There is virtually no downtime and no service disruption to clients because upgrades are rolled out seamlessly - one service at a time, rather than having to re-compile, re-build and re-start an entire monolithic application. As a result, businesses are able to develop and roll-out new features and updates a lot faster, in an agile approach, having separate teams focusing on separate features, thus being more productive and cost-effective. . Refactoring . A so-called “Big-bang” approach focuses all efforts with the refactoring of the monolith, postponing the development and implementation of any new features - essentially delaying progress and possibly, in the process, even breaking the core of the business, the monolith. . An incremental refactoring approach guarantees that new features are developed and implemented as modern microservices which are able to communicate with the monolith through APIs, without appending to the monolith’s code. In the meantime, features are refactored out of the monolith which slowly fades away while all, or most its functionality is modernized into microservices. This incremental approach offers a gradual transition from a legacy monolith to modern microservices architecture and allows for phased migration of application features into the cloud. . The refactoring phase slowly transforms the monolith into a cloud-native application which takes full advantage of cloud features, by coding in new programming languages and applying modern architectural patterns. Through refactoring, a legacy monolith application receives a second chance at life - to live on as a modular system adapted to fully integrate with today’s fast-paced cloud automation tools and services. . Challenges . When considering a legacy Mainframe based system, written in older programming languages - Cobol or Assembler, it may be more economical to just re-build it from the ground up as a cloud-native application. A poorly designed legacy application should be re-designed and re-built from scratch following modern architectural patterns for microservices and even containers. Applications tightly coupled with data stores are also poor candidates for refactoring. . Choosing runtimes may be another challenge. If deploying many modules on a single physical or virtual server, chances are that different libraries and runtime environment may conflict with one another causing errors and failures. This forces deployments of single modules per servers in order to separate their dependencies - not an economical way of resource management, and no real segregation of libraries and runtimes, as each server also has an underlying Operating System running with its libraries, thus consuming server resources - at times the OS consuming more resources than the application module itself. . Success Stories . Although a challenging process, moving from monoliths to microservices is a rewarding journey especially once a business starts to see growth and success delivered by a refactored application system. Below we are listing only a handful of the success stories of companies which rose to the challenge to modernize their monolith business applications. A detailed list of success stories is available at the Kubernetes website: Kubernetes User Case Studies. . AppDirect - an end-to-end commerce platform provider, started from a complex monolith application and through refactoring was able to retain limited functionality monoliths receiving very few commits, but all new features implemented as containerized microservices. . | box - a cloud storage solutions provider, started from a complex monolith architecture and through refactoring was able to decompose it into microservices. | | Crowdfire - a content management solutions provider, successfully broke down their initial monolith into microservices. | | GolfNow - a technology and services provider, decided to break their monoliths apart into containerized microservices. . | Pinterest - a social media services provider, started the refactoring process by first migrating their monolith API. | .",
    "url": "http://localhost:5000/docs/chapter1/",
    "relUrl": "/docs/chapter1/"
  }
  ,"2": {
    "title": "C2. Container Orchestration",
    "content": "⚠️ Notice: With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . Container Orchestration What Are Containers? | What Is Container Orchestration? | Container Orchestrators | Why Use Container Orchestrators? | Where to Deploy Container Orchestrators? | . | . Container Orchestration . With container images, we confine the application code, its runtime, and all of its dependencies in a pre-defined format. And, with container runtimes like runC, containerd, or rkt we can use those pre-packaged images, to create one or more containers. All of these runtimes are good at running containers on a single host. But, in practice, we would like to have a fault-tolerant and scalable solution, which can be achieved by creating a single controller/management unit, after connecting multiple nodes together. This controller/management unit is generally referred to as a container orchestrator. . In this chapter, we will explore why we should use container orchestrators, different implementations of container orchestrators, and where to deploy them. . What Are Containers? . Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications. . . Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies. . A container image bundles the application along with its runtime and dependencies, and a container is deployed from the container image offering an isolated executable environment for the application. Containers can be deployed from a specific image on many platforms, such as workstations, Virtual Machines, public cloud, etc. . What Is Container Orchestration? . In Development (Dev) environments, running containers on a single host for development and testing of applications may be an option. However, when migrating to Quality Assurance (QA) and Production (Prod) environments, that is no longer a viable option because the applications and services need to meet specific requirements: . Fault-tolerance | On-demand scalability | Optimal resource usage | Auto-discovery to automatically discover and communicate with each other | Accessibility from the outside world | Seamless updates/rollbacks without any downtime. | . Container orchestrators are tools which group systems together to form clusters where containers’ deployment and management is automated at scale while meeting the requirements mentioned above. . Container Orchestrators . With enterprises containerizing their applications and moving them to the cloud, there is a growing demand for container orchestration solutions. . Although not exhaustive, the list below provides a few different container orchestration tools and services available today: . Amazon Elastic Container Service Amazon Elastic Container Service (ECS) is a hosted service provided by Amazon Web Services (AWS) to run Docker containers at scale on its infrastructure. . | Azure Container Instances Azure Container Instance (ACI) is a basic container orchestration service provided by Microsoft Azure. . | Azure Service Fabric Azure Service Fabric is an open source container orchestrator provided by Microsoft Azure. . | Kubernetes Kubernetes is an open source orchestration tool, started by Google, part of the Cloud Native Computing Foundation (CNCF) project. . | Marathon Marathon is a framework to run containers at scale on Apache Mesos. . | Nomad Nomad is the container orchestrator provided by HashiCorp. . | Docker Swarm Docker Swarm is a container orchestrator provided by Docker, Inc. It is part of Docker Engine. . | . Why Use Container Orchestrators? . Although we can manually maintain a couple of containers or write scripts for dozens of containers, orchestrators make things much easier for operators especially when it comes to managing hundreds and thousands of containers running on a global infrastructure. . Most container orchestrators can: . Group hosts together while creating a cluster | Schedule containers to run on hosts in the cluster based on resources availability | Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster | Bind containers and storage resources | Group sets of similar containers and bind them to load-balancing constructs to simplify access to containerized applications by creating a level of abstraction between the containers and the user | Manage and optimize resource usage | Allow for implementation of policies to secure access to applications running inside containers. | . Where to Deploy Container Orchestrators? . Most container orchestrators can be deployed on the infrastructure of our choice - on bare metal, Virtual Machines, on-premise, or the public cloud. Kubernetes, for example, can be deployed on a workstation, with or without a local hypervisor such as Oracle VirtualBox, inside a company’s data center, in the cloud on AWS Elastic Compute Cloud (EC2) instances, Google Compute Engine (GCE) VMs, DigitalOcean Droplets, OpenStack, etc. . There are turnkey solutions which allow Kubernetes clusters to be installed, with only a few commands, on top of cloud Infrastructures-as-a-Service, such as GCE, AWS EC2, Docker Enterprise, IBM Cloud, Rancher, VMware, Pivotal, and multi-cloud solutions through IBM Cloud Private and StackPointCloud. . Last but not least, there is the managed container orchestration as-a-Service, more specifically the managed Kubernetes as-a-Service solution, offered and hosted by the major cloud providers, such as Google Kubernetes Engine (GKE), Amazon Elastic Container Service for Kubernetes (Amazon EKS), Azure Kubernetes Service (AKS), IBM Cloud Kubernetes Service, DigitalOcean Kubernetes, Oracle Container Engine for Kubernetes, etc. .",
    "url": "http://localhost:5000/docs/chapter2/",
    "relUrl": "/docs/chapter2/"
  }
  ,"3": {
    "title": "C3. Kubernetes",
    "content": "⚠️ Notice: With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . Kubernetes From Borg to Kubernetes | Kubernetes Features I | Kubernetes Features II | Why Use Kubernetes? | Cloud Native Computing Foundation (CNCF) | . | . Kubernetes . Definition from Kubernetes website: . “Kubernetes is an open-source system for automating deployment scaling, and management of containerized applications.” . From Borg to Kubernetes . According to the abstract of Google’s Borg paper, published in 2015, . “Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines”. . For more than a decade, Borg has been Google’s secret, running its worldwide containerized workloads in production. Services we use from Google, such as Gmail, Drive, Maps, Docs, etc., they are all serviced using Borg. . Some of the initial authors of Kubernetes were Google employees who have used Borg and developed it in the past. They poured in their valuable knowledge and experience while designing Kubernetes. Some of the features/objects of Kubernetes that can be traced back to Borg, or to lessons learned from it, are: . API servers | Pods | IP-per-Pod | Services | Labels. | . Kubernetes Features I . Kubernetes offers a very rich set of features for container orchestration. Some of its fully supported features are: . Automatic bin packing Kubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability. . | Self-healing Kubernetes automatically replaces and reschedules containers from failed nodes. It kills and restarts containers unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers. . | Horizontal scaling With Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization. . | Service discovery and Load balancing Containers receive their own IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set. . | . Kubernetes Features II . Automated rollouts and rollbacks Kubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application’s health to prevent any downtime. . | Secret and configuration management Kubernetes manages secrets and configuration details for an application separately from the container image, in order to avoid a re-build of the respective image. Secrets consist of confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub. . | Storage orchestration Kubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, or network storage systems. . | Batch execution Kubernetes supports batch execution, long-running jobs, and replaces failed containers. . | . Why Use Kubernetes? . In addition to its fully-supported features, Kubernetes is also portable and extensible. It can be deployed in many environments such as local or remote Virtual Machines, bare metal, or in public/private/hybrid/multi-cloud setups. It supports and it is supported by many 3rd party open source tools which enhance Kubernetes’ capabilities and provide a feature-rich experience to its users. . Kubernetes’ architecture is modular and pluggable. Not only that it orchestrates modular, decoupled microservices type applications, but also its architecture follows decoupled microservices patterns. Kubernetes’ functionality can be extended by writing custom resources, operators, custom APIs, scheduling rules or plugins. . Cloud Native Computing Foundation (CNCF) . The Cloud Native Computing Foundation (CNCF) is one of the projects hosted by the Linux Foundation. CNCF aims to accelerate the adoption of containers, microservices, and cloud-native applications. . You can observe all graduated and incubating projects from https://www.cncf.io/projects/ .",
    "url": "http://localhost:5000/docs/chapter3/",
    "relUrl": "/docs/chapter3/"
  }
  ,"4": {
    "title": "C4. Kubernetes Architecture",
    "content": "⚠️ Notice: With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . Kubernetes Architecture Master Node Master Node Components: API Server | Master Node Components: Scheduler | Master Node Components: Controller Managers | Master Node Components: etcd | . | Worker Node Worker Node Components: Container Runtime | Worker Node Components: kubelet | Worker Node Components: kubelet - CRI shims | Worker Node Components: kube-proxy | Worker Node Components: Addons | . | Networking Challenges | Container-to-Container Communication Inside Pods | Pod-to-Pod Communication Across Nodes | Pod-to-External World Communication | . | . Kubernetes Architecture . At a very high level, Kubernetes has the following main components: . One or more master nodes | One or more worker nodes | Distributed key-value store, such as etcd. | . . Master Node . The master node provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster’s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API). . . It is important to keep the control plane running at all costs. Losing the control plane may introduce downtimes, causing service disruption to clients, with possible loss of business. To ensure the control plane’s fault tolerance, master node replicas are added to the cluster, configured in High-Availability (HA) mode. While only one of the master node replicas actively manages the cluster, the control plane components stay in sync across the master node replicas. This type of configuration adds resiliency to the cluster’s control plane, should the active master node replica fail. . To persist the Kubernetes cluster’s state, all cluster configuration data is saved to etcd. However, etcd is a distributed key-value store which only holds cluster state related data, no client workload data. etcd is configured on the master node (stacked) or on its dedicated host (external) to reduce the chances of data store loss by decoupling it from the control plane agents. . When stacked, HA master node replicas ensure etcd resiliency as well. Unfortunately, that is not the case of external etcds, when the etcd hosts have to be separately replicated for HA mode configuration. . A master node has the following components: . API server | Scheduler | Controller managers | etcd. | . Master Node Components: API Server . All the administrative tasks are coordinated by the kube-apiserver, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster’s current state from the etcd, and after a call’s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster’s data store. . The API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules. . Master Node Components: Scheduler . The role of the kube-scheduler is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object’s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object’s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with disk==ssd key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc. . The scheduler is highly configurable and customizable. Additional custom schedulers are supported, then the object’s configuration data should include the name of the custom scheduler expected to make the scheduling decision for that particular object; if no such data is included, the default scheduler is selected instead. . A scheduler is extremely important and quite complex in a multi-node Kubernetes cluster. In a single-node Kubernetes cluster, such as the one explored later in this course, the scheduler’s job is quite simple. . Master Node Components: Controller Managers . controller managers . The controller managers are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster’s desired state (provided by objects’ configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state. . | kube-controller-manager . The kube-controller-manager runs controllers responsible to act when nodes become unavailable, to ensure pod counts are as expected, to create endpoints, service accounts, and API access tokens. . | cloud-controller-manager . The cloud-controller-manager runs controllers responsible to interact with the underlying infrastructure of a cloud provider when nodes become unavailable, to manage storage volumes when provided by a cloud service, and to manage load balancing and routing. . | . Master Node Components: etcd . etcd is a distributed key-value data store used to persist a Kubernetes cluster’s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store. . Out of all the control plane components, only the API server is able to communicate with the etcd data store. . etcd’s CLI management tool provides backup, snapshot, and restore capabilities which come in handy especially for a single etcd instance Kubernetes cluster - common in Development and learning environments. However, in Stage and Production environments, it is extremely important to replicate the data stores in HA mode, for cluster configuration data resiliency. . etcd is based on the Raft Consensus Algorithm which allows a collection of machines to work as a coherent group that can survive the failures of some of its members. At any given time, one of the nodes in the group will be the master, and the rest of them will be the followers. Any node can be treated as a master. . . etcd is written in the Go programming language. In Kubernetes, besides storing the cluster state, etcd is also used to store configuration details such as subnets, ConfigMaps, Secrets, etc. . Worker Node . Could be useful to check out POD definition. . A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together . . Also, to access the applications from the external world, we connect to worker nodes and not to the master node . A worker node has the following components: . Container runtime | kubelet | kube-proxy | Addons for DNS, Dashboard, cluster-level monitoring and logging. | . Worker Node Components: Container Runtime . Although Kubernetes is described as a “container orchestration engine”, it does not have the capability to directly handle containers. In order to run and manage a container’s lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Kubernetes supports many container runtimes: . Docker - although a container platform which uses containerd as a container runtime, it is the most widely used container runtime with Kubernetes . | CRI-O - a lightweight container runtime for Kubernetes, it also supports Docker image registries . | containerd - a simple and portable container runtime providing robustness . | rkt - a pod-native container engine, it also runs Docker images . | rktlet - a Kubernetes Container Runtime Interface (CRI) implementation using rkt . | . Worker Node Components: kubelet . The kubelet is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod’s running containers. . The kubelet connects to the container runtime using Container Runtime Interface (CRI). CRI consists of protocol buffers, gRPC API, and libraries. . . As shown above, the kubelet acting as grpc client connects to the CRI shim acting as grpc server to perform container and image operations. CRI implements two services: ImageService and RuntimeService. The ImageService is responsible for all the image-related operations, while the RuntimeService is responsible for all the Pod and container-related operations. . Container runtimes used to be hard-coded in Kubernetes, but with the development of CRI, Kubernetes is more flexible now and uses different container runtimes without the need to recompile. Any container runtime that implements CRI can be used by Kubernetes to manage Pods, containers, and container images. . Worker Node Components: kubelet - CRI shims . Below you will find some examples of CRI shims: . dockershim . With dockershim, containers are created using Docker installed on the worker nodes. Internally, Docker uses containerd to create and manage containers. . . | cri-containerd . With cri-containerd, we can directly use Docker’s smaller offspring containerd to create and manage containers. . . | CRI-O . CRI-O enables using any Open Container Initiative (OCI) compatible runtimes with Kubernetes. At the time this course was created, CRI-O supported runC and Clear Containers as container runtimes. However, in principle, any OCI-compliant runtime can be plugged-in. . . | . Worker Node Components: kube-proxy . The kube-proxy is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods. . Worker Node Components: Addons . Addons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services. . DNS - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources | Dashboard - a general purposed web-based user interface for cluster management | Monitoring - collects cluster-level container metrics and saves them to a central data store | Logging - collects cluster-level container logs and saves them to a central log store for analysis. | . Networking Challenges . Decoupled microservices based applications rely heavily on networking in order to mimic the tight-coupling once available in the monolithic era. Networking, in general, is not the easiest to understand and implement. Kubernetes is no exception - as a containerized microservices orchestrator is needs to address 4 distinct networking challenges: . Container-to-container communication inside Pods | Pod-to-Pod communication on the same node and across cluster nodes | Pod-to-Service communication within the same namespace and across cluster namespaces | External-to-Service communication for clients to access applications in a cluster. | . All these networking challenges must be addressed before deploying a Kubernetes cluster. . Container-to-Container Communication Inside Pods . Making use of the underlying host operating system’s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A network namespace is shared across containers, or with the host operating system. . When a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost. . Pod-to-Pod Communication Across Nodes . In a Kubernetes cluster Pods are scheduled on nodes randomly. Regardless of their host node, Pods are expected to be able to communicate with all other Pods in the cluster, all this without the implementation of Network Address Translation (NAT). This is a fundamental requirement of any networking implementation in Kubernetes. . The Kubernetes network model aims to reduce complexity, and it treats Pods as VMs on a network, where each VM receives an IP address - thus each Pod receiving an IP address. This model is called “IP-per-Pod” and ensures Pod-to-Pod communication, just as VMs are able to communicate with each other. . Let’s not forget about containers though. They share the Pod’s network namespace and must coordinate ports assignment inside the Pod just as applications would on a VM, all while being able to communicate with each other on localhost - inside the Pod. . However, containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI) supported by CNI plugins. . CNI is a set of a specification and libraries which allow plugins to configure the networking for containers. While there are a few core plugins, most CNI plugins are 3rd-party Software Defined Networking (SDN) solutions implementing the Kubernetes networking model. In addition to addressing the fundamental requirement of the networking model, some networking solutions offer support for Network Policies. Flannel, Weave, Calico are only a few of the SDN solutions available for Kubernetes clusters. . . The container runtime offloads the IP assignment to CNI, which connects to the underlying configured plugin, such as Bridge or MACvlan, to get the IP address. Once the IP address is given by the respective plugin, CNI forwards it back to the requested container runtime. . For more details, you can explore the Kubernetes documentation. . Pod-to-External World Communication . For a successfully deployed containerized applications running in Pods inside a Kubernetes cluster, it requires accessibility from the outside world. Kubernetes enables external accessibility through services, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with kube-proxy, applications become accessible from outside the cluster over a virtual IP. .",
    "url": "http://localhost:5000/docs/chapter4/",
    "relUrl": "/docs/chapter4/"
  }
  ,"5": {
    "title": "Introduction to Kubernetes",
    "content": "Introduction to Kubernetes . Important figures, notes and examples may be included in this website in order to do not lose the progress. . Go To Course . Go To Course . The course is conducted over edX for free if you do not want to have certificate for the course . . Link to course: https://www.edx.org/course/introduction-to-kubernetes . Course handouts can be downloaded here .",
    "url": "http://localhost:5000/",
    "relUrl": "/"
  }
  
}