{
  
  "1": {
    "title": "C1.From Monolith to Microservices",
    "content": "From Monolith to Microservices The Legacy Monolith | The Modern Service | Refactoring | Challenges | Success Stories | . | . From Monolith to Microservices . With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . The Legacy Monolith . Monolithic applications with all components tightly coupled and almost impossible to separate, a nightmare to manage and deployed on super expensive hardware. . Although most enterprises believe that the cloud will be the new home for legacy apps, not all legacy apps are a fit for the cloud, at least not yet. . Moving an application to the cloud should be as easy as walking on the beach and collecting pebbles in a bucket and easily carry them wherever needed. A 1000-ton boulder, on the other hand, is not easy to carry at all. This boulder represents the monolith application - sedimented layers of features and redundant logic translated into thousands of lines of code, written in a single, not so modern programming language, based on outdated software architecture patterns and principles. . In time, the new features and improvements added to code complexity, making development more challenging - loading, compiling, and building times increase with every new update. However, there is some ease in administration as the application is running on a single server, ideally a Virtual Machine or a Mainframe. . A monolith has a rather expensive taste in hardware. Being a large, single piece of software which continuously grows, it has to run on a single system which has to satisfy its compute, memory, storage, and networking requirements. The hardware of such capacity is both complex and pricey. . Since the entire monolith application runs as a single process, the scaling of individual features of the monolith is almost impossible. It internally supports a hardcoded number of connections and operations. However, scaling the entire application means to manually deploy a new instance of the monolith on another server, typically behind a load balancing appliance - another pricey solution. . During upgrades, patches or migrations of the monolith application - downtimes occur and maintenance windows have to be planned as disruptions in service are expected to impact clients. While there are solutions to minimize downtimes to customers by setting up monolith applications in a highly available active/passive configuration, it may still be challenging for system engineers to keep all systems at the same patch level. . The Modern Service . Microservices can be deployed individually on separate servers provisioned with fewer resources - only what is required by each service and the host system itself. . Microservices-based architecture is aligned with Event-driven Architecture and Service-Oriented Architecture (SOA) principles, where complex applications are composed of small independent processes which communicate with each other through APIs over a network. APIs allow access by other internal services of the same application or external, third-party services and applications. . Although the distributed nature of microservices adds complexity to the architecture, one of the greatest benefits of microservices is scalability. With the overall application becoming modular, each microservice can be scaled individually, either manually or automated through demand-based autoscaling. . Seamless upgrades and patching processes are other benefits of microservices architecture. There is virtually no downtime and no service disruption to clients because upgrades are rolled out seamlessly - one service at a time, rather than having to re-compile, re-build and re-start an entire monolithic application. As a result, businesses are able to develop and roll-out new features and updates a lot faster, in an agile approach, having separate teams focusing on separate features, thus being more productive and cost-effective. . Refactoring . A so-called “Big-bang” approach focuses all efforts with the refactoring of the monolith, postponing the development and implementation of any new features - essentially delaying progress and possibly, in the process, even breaking the core of the business, the monolith. . An incremental refactoring approach guarantees that new features are developed and implemented as modern microservices which are able to communicate with the monolith through APIs, without appending to the monolith’s code. In the meantime, features are refactored out of the monolith which slowly fades away while all, or most its functionality is modernized into microservices. This incremental approach offers a gradual transition from a legacy monolith to modern microservices architecture and allows for phased migration of application features into the cloud. . The refactoring phase slowly transforms the monolith into a cloud-native application which takes full advantage of cloud features, by coding in new programming languages and applying modern architectural patterns. Through refactoring, a legacy monolith application receives a second chance at life - to live on as a modular system adapted to fully integrate with today’s fast-paced cloud automation tools and services. . Challenges . When considering a legacy Mainframe based system, written in older programming languages - Cobol or Assembler, it may be more economical to just re-build it from the ground up as a cloud-native application. A poorly designed legacy application should be re-designed and re-built from scratch following modern architectural patterns for microservices and even containers. Applications tightly coupled with data stores are also poor candidates for refactoring. . Choosing runtimes may be another challenge. If deploying many modules on a single physical or virtual server, chances are that different libraries and runtime environment may conflict with one another causing errors and failures. This forces deployments of single modules per servers in order to separate their dependencies - not an economical way of resource management, and no real segregation of libraries and runtimes, as each server also has an underlying Operating System running with its libraries, thus consuming server resources - at times the OS consuming more resources than the application module itself. . Success Stories . Although a challenging process, moving from monoliths to microservices is a rewarding journey especially once a business starts to see growth and success delivered by a refactored application system. Below we are listing only a handful of the success stories of companies which rose to the challenge to modernize their monolith business applications. A detailed list of success stories is available at the Kubernetes website: Kubernetes User Case Studies. . AppDirect - an end-to-end commerce platform provider, started from a complex monolith application and through refactoring was able to retain limited functionality monoliths receiving very few commits, but all new features implemented as containerized microservices. . | box - a cloud storage solutions provider, started from a complex monolith architecture and through refactoring was able to decompose it into microservices. | | Crowdfire - a content management solutions provider, successfully broke down their initial monolith into microservices. | | GolfNow - a technology and services provider, decided to break their monoliths apart into containerized microservices. . | Pinterest - a social media services provider, started the refactoring process by first migrating their monolith API. | .",
    "url": "http://localhost:4000/docs/chapter1/",
    "relUrl": "/docs/chapter1/"
  }
  ,"2": {
    "title": "C10. Services",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter10/",
    "relUrl": "/docs/chapter10/"
  }
  ,"3": {
    "title": "C11. Deploying a Stand-Alone Application",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter11/",
    "relUrl": "/docs/chapter11/"
  }
  ,"4": {
    "title": "C12. Kubernetes Volume Management",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter12/",
    "relUrl": "/docs/chapter12/"
  }
  ,"5": {
    "title": "C13. ConfigMaps and Secrets",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter13/",
    "relUrl": "/docs/chapter13/"
  }
  ,"6": {
    "title": "C14. Ingress",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter14/",
    "relUrl": "/docs/chapter14/"
  }
  ,"7": {
    "title": "C15. Advanced Topics",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter15/",
    "relUrl": "/docs/chapter15/"
  }
  ,"8": {
    "title": "C16. Kubernetes Community",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter16/",
    "relUrl": "/docs/chapter16/"
  }
  ,"9": {
    "title": "C2. Container Orchestration",
    "content": "Container Orchestration What Are Containers? | What Is Container Orchestration? | Container Orchestrators | Why Use Container Orchestrators? | Where to Deploy Container Orchestrators? | . | . Container Orchestration . Notice: With respect to licence (https://creativecommons.org/licenses/by-nc-nd/4.0/) on this course, I am distributing and sharing important points and some information which are taken from https://www.edx.org/course/introduction-to-kubernetes course . With container images, we confine the application code, its runtime, and all of its dependencies in a pre-defined format. And, with container runtimes like runC, containerd, or rkt we can use those pre-packaged images, to create one or more containers. All of these runtimes are good at running containers on a single host. But, in practice, we would like to have a fault-tolerant and scalable solution, which can be achieved by creating a single controller/management unit, after connecting multiple nodes together. This controller/management unit is generally referred to as a container orchestrator. . In this chapter, we will explore why we should use container orchestrators, different implementations of container orchestrators, and where to deploy them. . What Are Containers? . Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications. . . Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies. . A container image bundles the application along with its runtime and dependencies, and a container is deployed from the container image offering an isolated executable environment for the application. Containers can be deployed from a specific image on many platforms, such as workstations, Virtual Machines, public cloud, etc. . What Is Container Orchestration? . In Development (Dev) environments, running containers on a single host for development and testing of applications may be an option. However, when migrating to Quality Assurance (QA) and Production (Prod) environments, that is no longer a viable option because the applications and services need to meet specific requirements: . Fault-tolerance | On-demand scalability | Optimal resource usage | Auto-discovery to automatically discover and communicate with each other | Accessibility from the outside world | Seamless updates/rollbacks without any downtime. | . Container orchestrators are tools which group systems together to form clusters where containers’ deployment and management is automated at scale while meeting the requirements mentioned above. . Container Orchestrators . With enterprises containerizing their applications and moving them to the cloud, there is a growing demand for container orchestration solutions. . Although not exhaustive, the list below provides a few different container orchestration tools and services available today: . Amazon Elastic Container Service Amazon Elastic Container Service (ECS) is a hosted service provided by Amazon Web Services (AWS) to run Docker containers at scale on its infrastructure. . | Azure Container Instances Azure Container Instance (ACI) is a basic container orchestration service provided by Microsoft Azure. . | Azure Service Fabric Azure Service Fabric is an open source container orchestrator provided by Microsoft Azure. . | Kubernetes Kubernetes is an open source orchestration tool, started by Google, part of the Cloud Native Computing Foundation (CNCF) project. . | Marathon Marathon is a framework to run containers at scale on Apache Mesos. . | Nomad Nomad is the container orchestrator provided by HashiCorp. . | Docker Swarm Docker Swarm is a container orchestrator provided by Docker, Inc. It is part of Docker Engine. . | . Why Use Container Orchestrators? . Although we can manually maintain a couple of containers or write scripts for dozens of containers, orchestrators make things much easier for operators especially when it comes to managing hundreds and thousands of containers running on a global infrastructure. . Most container orchestrators can: . Group hosts together while creating a cluster | Schedule containers to run on hosts in the cluster based on resources availability | Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster | Bind containers and storage resources | Group sets of similar containers and bind them to load-balancing constructs to simplify access to containerized applications by creating a level of abstraction between the containers and the user | Manage and optimize resource usage | Allow for implementation of policies to secure access to applications running inside containers. | . Where to Deploy Container Orchestrators? . Most container orchestrators can be deployed on the infrastructure of our choice - on bare metal, Virtual Machines, on-premise, or the public cloud. Kubernetes, for example, can be deployed on a workstation, with or without a local hypervisor such as Oracle VirtualBox, inside a company’s data center, in the cloud on AWS Elastic Compute Cloud (EC2) instances, Google Compute Engine (GCE) VMs, DigitalOcean Droplets, OpenStack, etc. . There are turnkey solutions which allow Kubernetes clusters to be installed, with only a few commands, on top of cloud Infrastructures-as-a-Service, such as GCE, AWS EC2, Docker Enterprise, IBM Cloud, Rancher, VMware, Pivotal, and multi-cloud solutions through IBM Cloud Private and StackPointCloud. . Last but not least, there is the managed container orchestration as-a-Service, more specifically the managed Kubernetes as-a-Service solution, offered and hosted by the major cloud providers, such as Google Kubernetes Engine (GKE), Amazon Elastic Container Service for Kubernetes (Amazon EKS), Azure Kubernetes Service (AKS), IBM Cloud Kubernetes Service, DigitalOcean Kubernetes, Oracle Container Engine for Kubernetes, etc. .",
    "url": "http://localhost:4000/docs/chapter2/",
    "relUrl": "/docs/chapter2/"
  }
  ,"10": {
    "title": "C3. Kubernetes",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter3/",
    "relUrl": "/docs/chapter3/"
  }
  ,"11": {
    "title": "C4. Kubernetes Architecture",
    "content": "Content will be added ! .",
    "url": "http://localhost:4000/docs/chapter4/",
    "relUrl": "/docs/chapter4/"
  }
  ,"12": {
    "title": "C5. Installing Kubernetes",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter5/",
    "relUrl": "/docs/chapter5/"
  }
  ,"13": {
    "title": "C6. Minikube",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter6/",
    "relUrl": "/docs/chapter6/"
  }
  ,"14": {
    "title": "C7. Accessing Minikube",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter7/",
    "relUrl": "/docs/chapter7/"
  }
  ,"15": {
    "title": "C8. Kubernetes Building Blocks",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter8/",
    "relUrl": "/docs/chapter8/"
  }
  ,"16": {
    "title": "C9. Authentication, Authorization",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/chapter9/",
    "relUrl": "/docs/chapter9/"
  }
  ,"17": {
    "title": "Final Exam",
    "content": "Content will be added .",
    "url": "http://localhost:4000/docs/final/",
    "relUrl": "/docs/final/"
  }
  ,"18": {
    "title": "Introduction to Kubernetes",
    "content": "Introduction to Kubernetes . Important figures, notes and examples may be included in this website in order to do not lose the progress. . Go To Course . Go To Course . The course is conducted over edX for free if you do not want to have certificate for the course . . Link to course: https://www.edx.org/course/introduction-to-kubernetes .",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
  
}